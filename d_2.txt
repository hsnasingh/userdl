# Assignment 6 and 7: Facial Detection and gesture recognition

# pip install deepface

from deepface import DeepFace
import cv2
import matplotlib.pyplot as plt

# Load the image
img_path = '/content/grp photo.jpg'  # Replace with your image path
img = cv2.imread(img_path)

# Detect faces and analyze emotions
results = DeepFace.analyze(img_path, actions=["emotion"], enforce_detection=False)    #only analyze and no face detection.

# Ensure results is always a list for consistent processing
if not isinstance(results, list):
    results = [results]

# Draw bounding boxes and display emotions
for face in results:
    # Get face region coordinates
    region = face['region']
    x, y, w, h = region['x'], region['y'], region['w'], region['h']
    emotion = face['dominant_emotion'] # Get the dominant emotion and draw it on the image

    # Draw bounding box around the face
    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
    cv2.putText(img, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Show the image with detections and emotions
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()








# Assignment 8:  Image denoising

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D
import cv2

# Load and preprocess data
(x_train, _), (_, _) = mnist.load_data()
x_train = x_train / 255.0  # Normalize
x_train = x_train[..., np.newaxis]  # Add channel    #adding a new dimension to the data.

# Add noise to the training data
noise = 0.5 * np.random.normal(size=x_train.shape)     #adds random gaussian noise to the image of the same shape as x_train 
x_train_noisy = np.clip(x_train + noise, 0, 1)         #the noisy image is clipped to ensure that the pixel values are in between 0 and 1.

# Build autoencoder
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    UpSampling2D((2, 2)),
    Conv2D(1, (3, 3), activation='sigmoid', padding='same')
])

# Train model
model.compile(optimizer='adam', loss='mse')
model.fit(x_train_noisy, x_train, epochs=2, batch_size=64)         #The model is trained on the noisy training data (x_train_noisy) with the original clean data (x_train) as the target.

# Function to process user-provided image
def process_user_image():
    path = input("Enter image path: ")
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, (28, 28)) / 255.0  # Resize and normalize
    noise = 0.5 * np.random.normal(size=img.shape)
    img_noisy = np.clip(img + noise, 0, 1)
    img_noisy = img_noisy[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions
    denoised = model.predict(img_noisy)

    # Display the images
    plt.subplot(1, 3, 1)
    plt.imshow(img, cmap='gray')
    plt.title("Original")
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(img_noisy.squeeze(), cmap='gray')
    plt.title("Noisy")
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(denoised.squeeze(), cmap='gray')
    plt.title("Denoised")
    plt.axis('off')
    plt.show()

# Take user input image
process_user_image()






#  Assignment 9:  Semantic segmentation

# Import required libraries
import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate
from tensorflow.keras.optimizers import Adam
from google.colab import files
import os
from IPython.display import display, clear_output

# Define the model
def create_segmentation_model(input_size=(256, 256, 3), n_classes=4):
    inputs = Input(input_size)

    # Encoder
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    # Bridge
    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)
    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)

    # Decoder
    up5 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv3], axis=3)
    conv5 = Conv2D(256, (3, 3), activation='relu', padding='same')(up5)
    conv5 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv5)

    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv2], axis=3)
    conv6 = Conv2D(128, (3, 3), activation='relu', padding='same')(up6)
    conv6 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv6)

    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv1], axis=3)
    conv7 = Conv2D(64, (3, 3), activation='relu', padding='same')(up7)
    conv7 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv7)

    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(conv7)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-4),
                 loss='categorical_crossentropy',
                 metrics=['accuracy'])

    return model

# Image preprocessing function
def preprocess_image(image):
    # Resize image
    image = cv2.resize(image, (256, 256))

    # Convert to RGB if needed
    if len(image.shape) == 2:
        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    elif image.shape[2] == 4:
        image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)
    elif image.shape[2] == 3:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Normalize
    image = image / 255.0
    return image

# Function to create colored segmentation mask
def create_segmentation_mask(prediction):
    # Define colors for each class
    colors = [
        [0, 0, 0],       # Background (black)
        [255, 0, 0],     # Traffic Signal (red)
        [128, 128, 128], # Road (gray)
        [0, 255, 0]      # Vehicle (green)
    ]

    # Convert prediction to color mask
    prediction = np.argmax(prediction[0], axis=-1)
    mask = np.zeros((256, 256, 3), dtype=np.uint8)

    for i, color in enumerate(colors):
        mask[prediction == i] = color

    return mask

# Function to display results
def display_results(original_image, segmentation_mask):
    plt.figure(figsize=(15, 5))

    # Display original image
    plt.subplot(131)
    plt.imshow(original_image)
    plt.title('Original Image')
    plt.axis('off')

    # Display segmentation mask
    plt.subplot(132)
    plt.imshow(segmentation_mask)
    plt.title('Segmentation Result')
    plt.axis('off')

    # Display overlay
    overlay = cv2.addWeighted(
        (original_image * 255).astype(np.uint8),
        0.7,
        segmentation_mask,
        0.3,
        0
    )
    plt.subplot(133)
    plt.imshow(overlay)
    plt.title('Overlay')
    plt.axis('off')

    # Add legend
    legend_elements = [
        plt.Rectangle((0,0),1,1, facecolor='black'),
        plt.Rectangle((0,0),1,1, facecolor='red'),
        plt.Rectangle((0,0),1,1, facecolor='gray'),
        plt.Rectangle((0,0),1,1, facecolor='green')
    ]
    plt.figlegend(
        legend_elements,
        ['Background', 'Traffic Signal', 'Road', 'Vehicle'],
        loc='center right',
        bbox_to_anchor=(1.2, 0.5)
    )

    plt.tight_layout()
    plt.show()

# Main detection function
def detect_traffic_signals():
    print("Select an image to upload...")
    uploaded = files.upload()

    if len(uploaded) == 0:
        print("No file uploaded!")
        return

    # Get the uploaded file
    image_path = next(iter(uploaded))

    try:
        # Read image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError("Could not read the image!")

        # Preprocess image
        processed_image = preprocess_image(image)

        # Create and compile model
        model = create_segmentation_model()

        # In a real application, you would load pre-trained weights here
        # model.load_weights('pretrained_weights.h5')

        # Make prediction
        prediction = model.predict(np.expand_dims(processed_image, axis=0))

        # Create segmentation mask
        segmentation_mask = create_segmentation_mask(prediction)

        # Display results
        display_results(processed_image, segmentation_mask)

        # Calculate and display statistics
        prediction_classes = np.argmax(prediction[0], axis=-1)
        class_names = ['Background', 'Traffic Signal', 'Road', 'Vehicle']

        print("\nDetection Statistics:")
        print("--------------------")
        for i, class_name in enumerate(class_names):
            percentage = np.mean(prediction_classes == i) * 100
            print(f"{class_name}: {percentage:.2f}% of image")

    except Exception as e:
        print(f"Error during detection: {str(e)}")

    finally:
        # Cleanup
        if os.path.exists(image_path):
            os.remove(image_path)

# Run this in Colab to start the application
print("Traffic Signal Detection System")
print("------------------------------")
print("This system will detect and segment:")
print("1. Traffic Signals (Red)")
print("2. Roads (Gray)")
print("3. Vehicles (Green)")
print("4. Background (Black)")
print("\nClick 'Choose Files' when the upload button appears...")

# Start detection
detect_traffic_signals()






# Assignment 10:  Image classification

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values
x_train = x_train[..., np.newaxis]  # Add channel dimension (28, 28, 1)
x_test = x_test[..., np.newaxis]

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # 10 classes for digits 0-9
])

# Compile and train the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=3, batch_size=64)

# Class labels for MNIST
class_names = [str(i) for i in range(10)]  # Class labels for digits 0-9

# Function to predict user-provided image
def predict_image():
    path = input("Enter image path: ")
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale
    img_resized = cv2.resize(img, (28, 28)) / 255.0  # Resize and normalize

    plt.imshow(img_resized, cmap='gray')  # Display the image
    plt.axis('off')
    plt.show()

    # Prepare the image for prediction
    img_resized = img_resized[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions
    prediction = model.predict(img_resized).argmax()
    print(f"Predicted Class: {class_names[prediction]}")  # Print the predicted class

# Predict on a new image
predict_image()







# Assignment 13:  LSTM

# Sentiment Analysis using LSTM

import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load and preprocess dataset
df = pd.read_csv(r'C:\Users\Manthan\Desktop\CV_DL_Practicals\sentiment_analysis.csv')
df = df.dropna(subset=['text', 'sentiment'])  # Drop missing values
df['text'] = df['text'].astype(str)

# Encode labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['sentiment'])

# Tokenize and pad sequences
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(df['text'])
sequences = tokenizer.texts_to_sequences(df['text'])
X = pad_sequences(sequences, maxlen=1000, padding='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build and compile the LSTM model
model = Sequential([
    Embedding(10000, 128, input_length=1000),
    Bidirectional(LSTM(64, return_sequences=True)),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(label_encoder.classes_), activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))

# Predict sentiment of new text
def predict_sentiment(text):
    tokens = tokenizer.texts_to_sequences([text])
    padded_text = pad_sequences(tokens, maxlen=1000, padding='post')
    prediction = model.predict(padded_text).argmax()
    sentiment = label_encoder.inverse_transform([prediction])[0]
    print(f"Text: {text}\nSentiment: {sentiment}")

# Example predictions
predict_sentiment("This game is amazing!")
predict_sentiment("The service was terrible.")
predict_sentiment("It's okay, nothing special.")
